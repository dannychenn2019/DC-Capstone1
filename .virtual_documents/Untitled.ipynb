import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as stats
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder

# Define the file path
file_path = "/Users/dannychen/Downloads/student_performance_dataset.csv"

# Load the CSV file into a DataFrame
df = pd.read_csv(file_path)


# Drop rows with missing target values 
df = df.dropna(subset=["Final_Exam_Score"])

# Fill missing values for other columns if necessary
df.fillna(df.median(numeric_only=True), inplace=True)


# Convert categorical columns to category type
categorical_cols = ["Parental_Education_Level", "Internet_Access_at_Home", "Extracurricular_Activities", "Gender"]
for col in categorical_cols:
    df[col] = df[col].astype("category")


# Distribution plots for numeric features
numeric_cols = ["Study_Hours_per_Week", "Attendance_Rate", "Past_Exam_Scores", "Final_Exam_Score"]
for col in numeric_cols:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[col], bins=20, kde=True)
    plt.title(f"Distribution of {col}")
    plt.show()


# Count plots for categorical features
for col in categorical_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(x=col, data=df)
    plt.title(f"Count Plot for {col}")
    plt.show()


# Scatter plots with regression lines for numeric features
for col in numeric_cols[:-1]:
    plt.figure(figsize=(8, 4))
    sns.regplot(x=col, y="Final_Exam_Score", data=df, line_kws={"color": "red"})
    plt.title(f"{col} vs Final Exam Score")
    plt.show()


# Correlation heatmap for numeric features 
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap for Numeric Features")
plt.show()


# Feature Importance using Random Forest
# Encode categorical variables for Random Forest
df_encoded = df.copy()
for col in categorical_cols:
    le = LabelEncoder()
    df_encoded[col] = le.fit_transform(df_encoded[col])

X = df_encoded[numeric_cols[:-1] + categorical_cols]
y = df_encoded["Final_Exam_Score"]

model = RandomForestRegressor(random_state=42)
model.fit(X, y)
feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)


# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances, y=feature_importances.index)
plt.title("Feature Importance from Random Forest")
plt.show()


# Predictive Modeling

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

# Define the file path
file_path = "/Users/dannychen/Downloads/student_performance_dataset.csv"

# Load the CSV file into a DataFrame
df = pd.read_csv(file_path)


# Drop rows with missing target values if any
df = df.dropna(subset=["Final_Exam_Score"])

# Fill missing values for other columns if necessary
df.fillna(df.median(numeric_only=True), inplace=True)


# Identify non-numeric columns (including the unexpected 'S158')
non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()
print("Non-numeric columns identified:", non_numeric_cols)

# Encode all non-numeric columns
le = LabelEncoder()
for col in non_numeric_cols:
    df[col] = le.fit_transform(df[col])


# Define features and target
X = df.drop(columns=["Final_Exam_Score"])
y = df["Final_Exam_Score"]


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize numeric features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# Function to evaluate models
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    return r2, mae, rmse


# Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
lr_r2, lr_mae, lr_rmse = evaluate_model(lr, X_test, y_test)

# Random Forest Regressor
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
rf_r2, rf_mae, rf_rmse = evaluate_model(rf, X_test, y_test)

# K-Nearest Neighbors Regressor
knn = KNeighborsRegressor()
knn.fit(X_train, y_train)
knn_r2, knn_mae, knn_rmse = evaluate_model(knn, X_test, y_test)



# Display results
results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "KNN Regressor"],
    "R² Score": [lr_r2, rf_r2, knn_r2],
    "MAE": [lr_mae, rf_mae, knn_mae],
    "RMSE": [lr_rmse, rf_rmse, knn_rmse]
})

print("\nModel Performance Comparison:")
print(results.sort_values(by="R² Score", ascending=False))
